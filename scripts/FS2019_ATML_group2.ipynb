{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATML Project Report\n",
    "\n",
    "### Group 2\n",
    "Members: LÃ©onard Barras & Nathan Gyger\n",
    "\n",
    "Github: https://github.com/svapili/FS2019_ATML_Group2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melanoma image classification\n",
    "### Goal:\n",
    "Implement a deep learning algorithm to classify mole pictures as benign or malignant using the [ISIC database](https://isic-archive.com/).\n",
    "\n",
    "### Approach description:\n",
    "\n",
    "- Class imbalance => data augmentation\n",
    "- Images => CNN as a logical choice\n",
    "- Transfer learning\n",
    "\n",
    "### Procedure\n",
    "\n",
    "- Augment the data for better balancing of classes (50-50 /33-67 ?)\n",
    "- Train different CNN on a few epochs with different optimizer, learning rate and batch sizes.\n",
    "- Select the more performant regarding different metrics: Accuracy, TP, TN ... Be sure that the problem of class unbalance has been corrected\n",
    "- ? Fine tune ?\n",
    "- ? Modify the argmax threshold in the output, Plot ROC Curve for different models ?\n",
    "- Modify data augmentation for getting a better specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "TODO: add accuracy table for different parameters\n",
    "\n",
    "## Learning curve\n",
    "TODO: add graphic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Hello from cluster!\")\n",
    "print(\"Available GPU: \", torch.cuda.get_device_name(0))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import util\n",
    "import csv\n",
    "import glob\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom functions\n",
    "import dataSplitter\n",
    "import loader\n",
    "import dataAugmenter\n",
    "import SimpleNet\n",
    "import train\n",
    "import test_\n",
    "import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths definitions\n",
    "cluster = False\n",
    "if cluster:\n",
    "    Path = '/var/tmp/'\n",
    "else:\n",
    "    Path = '../data/'\n",
    "dataDir = Path + 'ISIC-images'\n",
    "trainDir = Path + 'ISIC-images/train/'\n",
    "testDir = Path + 'ISIC-images/test/'\n",
    "valDir = Path + 'ISIC-images/val/'\n",
    "\n",
    "\n",
    "# Paths definitions for saving results and model state\n",
    "my_path = os.getcwd()\n",
    "dir = os.path.dirname(my_path)\n",
    "results_dir = dir + '/results'\n",
    "modelstate_dir = '/var/tmp/modelstate'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "if not os.path.exists(modelstate_dir):\n",
    "    os.makedirs(modelstate_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "# Data have first to be downloaded with data_downloader.py and extracted with data_extractor.py\n",
    "newDataSplit = False # Set to true to split the data randomly again\n",
    "dataPreprocessing = False # Set to true to resize and augment the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we can use CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a new random split of the data\n",
    "if (newDataSplit):\n",
    "    testRatio = .1\n",
    "    valRatio = .1\n",
    "    dataSplitter.split(trainDir, testDir, valDir, testRatio, valRatio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data (resizing and augmenting)\n",
    "if (dataPreprocessing):\n",
    "    #dataAugmenter.preprocessData([trainDir, testDir, valDir])\n",
    "    dataAugmenter.preprocessData([trainDir, testDir, valDir], outSize=(300,300), keepAspectRatio=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloaders objects\n",
    "image_datasets, dataloaders = loader.melanomaDataLoader(dataDir, batch_size=batch_size)\n",
    "\n",
    "# Get dataset objects sizes\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test', 'val']}\n",
    "print(\"Size of the dataset objects: \", dataset_sizes)\n",
    "\n",
    "# Get the class names\n",
    "class_names = image_datasets['train'].classes\n",
    "print(\"Images class names: \", class_names)\n",
    "\n",
    "# Visualize sample images\n",
    "print(\"Sample images:\")\n",
    "loader.showSample(dataloaders, dataset_sizes, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# SELECT MODEL\n",
    "###############################\n",
    "pretrained = False\n",
    "\n",
    "#model = models.alexnet(pretrained=True)\n",
    "#model = models.AlexNet(num_classes=2)\n",
    "model = models.resnet18(pretrained=pretrained) \n",
    "#model = models.resnet50(pretrained=pretrained)\n",
    "#model = models.densenet201()\n",
    "\n",
    "###############################\n",
    "# SELECT OPTIMIZER\n",
    "###############################\n",
    "finetune = True # Only has an effect if pretrained==True. If finetune==False, then fixed-feature\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer_name=\"Adam\" # \"Adam\" or \"SGD\"\n",
    "        \n",
    "optimizer = optimize.setup_optimizer(model, learning_rate, optimizer_name, pretrained, finetune)\n",
    "\n",
    "###############################\n",
    "# SELECT SCHEDULER\n",
    "###############################\n",
    "schedule = True\n",
    "step_size = 10\n",
    "gamma = 0.1\n",
    "\n",
    "if schedule:\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma)\n",
    "else:\n",
    "    scheduler = None\n",
    "    \n",
    "###############################\n",
    "# SELECT EARLY STOPPING\n",
    "###############################\n",
    "earlyStop = True\n",
    "\n",
    "if earlyStop:\n",
    "    best_val_loss = np.inf\n",
    "    best_model = None\n",
    "    max_epochs = 5 # if no improvement after 5 epochs, stop training\n",
    "    counter = 0\n",
    "\n",
    "###############################\n",
    "# SELECT LOSS FUNCTION\n",
    "###############################\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 30\n",
    "debug_training_status = False\n",
    "\n",
    "\n",
    "saving = True\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Epoch Training Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test train and test function\n",
    "if debug_training_status is True:\n",
    "    train_loss, train_accuracy = train.train(model, dataloaders['train'], optimizer, loss_fn, device, status = debug_training_status)\n",
    "    val_loss, val_accuracy, TP, TN, FP, FN  = test_.test(model, dataloaders['val'], loss_fn, device)\n",
    "    test_loss, test_accuracy, TP, TN, FP, FN = test_.test(model, dataloaders['test'], loss_fn, device)\n",
    "    print('Test training: train_loss: {:.4f}, train_accuracy: {:.4f}, val_loss: {:.4f}, val_accuracy: {:.4f}, test_loss: {:.4f}, test_accuracy: {:.4f}'.format(\n",
    "        train_loss,\n",
    "        train_accuracy,\n",
    "        val_loss,\n",
    "        val_accuracy,\n",
    "        test_loss,\n",
    "        test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import time\n",
    "    \n",
    "    train_losses, train_accuracies = ['train_losses'], ['train_accuracies']\n",
    "    val_losses, val_accuracies = ['val_losses'], ['val_accuracies']\n",
    "    learn_rates = ['learning_rate']\n",
    "    time_epoch = ['execution time']\n",
    "    \n",
    "    TPs = ['True Positives']\n",
    "    TNs = ['True Negatives']\n",
    "    FPs = ['False Positives']\n",
    "    FNs = ['False Negatives']\n",
    "    \n",
    "    config  = model._get_name() + \" \" + \"_bs=\" + str(batch_size)\n",
    "    \n",
    "    ##############################\n",
    "    # Training Epochs            #\n",
    "    ##############################\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time_epoch = time.time()\n",
    "        \n",
    "        train_loss, train_accuracy = train.train(model, dataloaders['train'], optimizer, loss_fn, device)\n",
    "        val_loss, val_accuracy, TP, TN, FP, FN  = test_.test(model, dataloaders['val'], loss_fn, device) \n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        TPs.append(TP)\n",
    "        TNs.append(TN)\n",
    "        FPs.append(FP)\n",
    "        FNs.append(FN)\n",
    "        \n",
    "        # SCHEDULER\n",
    "        learn_rates.append(optimizer.param_groups[0]['lr'])\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # TIME CALCULATION\n",
    "        time_last_epoch = time.time() - start_time_epoch\n",
    "        time_epoch.append(time_last_epoch)\n",
    "        \n",
    "        # OTHER METRICS\n",
    "        \n",
    "        \n",
    "        print('Epoch {}/{}: train_loss: {:.4f}, train_accuracy: {:.4f}, val_loss: {:.4f}, val_accuracy: {:.4f}, learn_rates: {:}, epoch execution time: {:.4f}'.format(\n",
    "            epoch + 1, n_epochs,\n",
    "            train_losses[-1],\n",
    "            train_accuracies[-1],\n",
    "            val_losses[-1],\n",
    "            val_accuracies[-1],\n",
    "            learn_rates[-1],\n",
    "            time_epoch[-1]))\n",
    "        \n",
    "        print('True Positive: {}, True Negative: {}, False Positives: {}, False Negative: {}'.format(\n",
    "            TPs[-1],\n",
    "            TNs[-1],\n",
    "            FPs[-1],\n",
    "            FNs[-1]))\n",
    "\n",
    "    ##############################\n",
    "    # Saving results             #\n",
    "    ##############################\n",
    "\n",
    "        if saving is True: #and (epoch+1) % 5 == 0:\n",
    "            print('...saving...')\n",
    "            name = config + '_' + loss_fn.__str__() + '_lr=' + str(learning_rate) + '_' +(optimizer.__str__()).split(' ')[0]\n",
    "\n",
    "            #remove old results\n",
    "            for filename in glob.glob(results_dir + '/' + name + '*'):\n",
    "                os.remove(filename)\n",
    "            for filename in glob.glob(modelstate_dir + '/' + name + '*'):\n",
    "                os.remove(filename)\n",
    "\n",
    "            name = name + '_Epoch_' + str(epoch+1)\n",
    "\n",
    "            # save model weights\n",
    "            torch.save(model.state_dict(), modelstate_dir + '/' + name + '.pth')\n",
    "\n",
    "            # save results per epoch\n",
    "            path = results_dir + '/' + name + '.csv'\n",
    "            with open(path, 'a') as csvFile:\n",
    "                writer = csv.writer(csvFile)\n",
    "                writer.writerow(train_losses)\n",
    "                writer.writerow(train_accuracies)\n",
    "                writer.writerow(val_losses)\n",
    "                writer.writerow(val_accuracies)\n",
    "                writer.writerow(learn_rates)\n",
    "                writer.writerow(time_epoch)\n",
    "                writer.writerow(TPs)\n",
    "                writer.writerow(TNs)\n",
    "                writer.writerow(FPs)\n",
    "                writer.writerow(FNs)\n",
    "            csvFile.close()\n",
    "            \n",
    "    ##############################\n",
    "    # Early stopping             #\n",
    "    ##############################\n",
    "        if earlyStop:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = deepcopy(model)\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "            if counter == max_epochs:\n",
    "                print('No improvement for {} epochs; training stopped.'.format(max_epochs))\n",
    "                \n",
    "                print('...saving...')\n",
    "                name = config + '_' + loss_fn.__str__() + '_lr=' + str(learning_rate) + '_' +(optimizer.__str__()).split(' ')[0]\n",
    "\n",
    "                #remove old results\n",
    "                for filename in glob.glob(results_dir + '/' + name + '*'):\n",
    "                    os.remove(filename)\n",
    "                for filename in glob.glob(modelstate_dir + '/' + name + '*'):\n",
    "                    os.remove(filename)\n",
    "\n",
    "                name = name + '_Epoch_' + str(epoch+1)\n",
    "\n",
    "                # save model weights\n",
    "                torch.save(best_model.state_dict(), modelstate_dir + '/' + name + '.pth')\n",
    "\n",
    "                # save results per epoch\n",
    "                path = results_dir + '/' + name + '.csv'\n",
    "                with open(path, 'a') as csvFile:\n",
    "                    writer = csv.writer(csvFile)\n",
    "                    writer.writerow(train_losses)\n",
    "                    writer.writerow(train_accuracies)\n",
    "                    writer.writerow(val_losses)\n",
    "                    writer.writerow(val_accuracies)\n",
    "                    writer.writerow(learn_rates)\n",
    "                    writer.writerow(time_epoch)\n",
    "                    writer.writerow(TPs)\n",
    "                    writer.writerow(TNs)\n",
    "                    writer.writerow(FPs)\n",
    "                    writer.writerow(FNs)\n",
    "                csvFile.close()\n",
    "                \n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first attempt, we tried to find out which model could provide the best results. Different models were tested with two different optimizers (SGD without momentum and Adam). Different learning rate were applied, deformed images were used and no pretraining was selected. Different batch_sizes were tested and the models were trained only on a limited number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing files beginning with an underline (i.e. \"_\")\n",
    "files_ = glob.glob(results_dir + \"/_*.csv\", recursive=True)\n",
    "files_.sort()\n",
    "print('Configuration \\t'.expandtabs(60), 'Epochs \\t', 'Accuracy \\t' , 'Specificity \\t Sensitivity')\n",
    "for f in files_:\n",
    "    file_name = f.rsplit('/')[-1]\n",
    "    parameters = file_name.split('_')\n",
    "    netname = parameters[1]\n",
    "    batch_size = parameters[2]\n",
    "    loss_fn = parameters[3]\n",
    "    lr = parameters[4]\n",
    "    optimizer = parameters[5]\n",
    "    epochs_s = parameters[6]\n",
    "    epoch_nr = parameters[7].split('.')[0]\n",
    "    if lr == \"lr=0.001\" and batch_size==\"bs=8\":\n",
    "        with open(f) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            list_training = []\n",
    "            for row in csv_reader:\n",
    "                list_training.append(row)\n",
    "            train_losses = list_training[0]\n",
    "            train_accuracies = list_training[1]\n",
    "            val_losses = list_training[2]\n",
    "            val_accuracies = list_training[3]\n",
    "\n",
    "            time = list_training[4]\n",
    "\n",
    "            TP = int(list_training[5][-1])\n",
    "            TN = int(list_training[6][-1])\n",
    "            FP = int(list_training[7][-1])\n",
    "            FN = int(list_training[8][-1])\n",
    "\n",
    "            specificity = TP/(TP+FN)\n",
    "            sensitivity = TN/(TN+FP)\n",
    "\n",
    "            epochs = list(range(1,len(val_accuracies)))\n",
    "\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(netname, batch_size, loss_fn, lr, '\\t', optimizer, '\\t|\\t', str(2), '\\t|\\t', val_accuracies[2][0:4], '\\t|\\t', str(specificity)[0:4], '\\t|\\t', str(sensitivity)[0:4])\n",
    "    #print(\"%.2f\" % (float(val_accuracies[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find out that the ResNet18 and Resnet50 provide the best results as well as a reasonable calculation time. A simple home-made model was also tested but it was discarded due to poor performances. A DenseNet201 model was also tested and was promising, but it was discarded due to the consequent amount of time required for its training.\n",
    "\n",
    "In particular the two best results were:\n",
    "\n",
    "ResNet18  bs=8 CrossEntropyLoss() lr=0.001 Adam Epoch \t|\t 82.0 \t|\t 0.59 \t|\t 0.90\n",
    "\n",
    "ResNet50  bs=8 CrossEntropyLoss() lr=0.001 Adam Epoch \t|\t 82.4 \t|\t 0.74 \t|\t 0.86\n",
    "\n",
    "From there, we've tried to improve the performances of both the ResNet18 and the ResNet50 models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare files beginning with an hypen (i.e \"-\")\n",
    "files = glob.glob(results_dir + \"/-*.csv\", recursive=True)\n",
    "files.sort()\n",
    "print('Configuration \\t'.expandtabs(58), 'Pretrained \\t', 'Epochs \\t', 'Accuracy \\t' , 'Specificity \\t Sensitivity', '\\t Pre-processing')\n",
    "for f in files:\n",
    "    file_name = f.rsplit('/')[-1]\n",
    "    parameters = file_name.split('_')\n",
    "    netname = parameters[0].split('-')[1]\n",
    "    batch_size = parameters[1]\n",
    "    loss_fn = parameters[2]\n",
    "    lr = parameters[3]\n",
    "    optimizer = parameters[4]\n",
    "    epochs_s = parameters[5]\n",
    "    epoch_nr = parameters[6]\n",
    "    preprocessing = parameters[7]\n",
    "    pretrained = parameters[8].split('.')[0]\n",
    "    if lr == \"lr=0.001\" and batch_size==\"bs=8\":\n",
    "        with open(f) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            list_training = []\n",
    "            for row in csv_reader:\n",
    "                list_training.append(row)\n",
    "            train_losses = list_training[0]\n",
    "            train_accuracies = list_training[2]\n",
    "            val_losses = list_training[4]\n",
    "            val_accuracies = list_training[6]\n",
    "            learning_rate = list_training[8]\n",
    "            \n",
    "            time = list_training[10]\n",
    "                        \n",
    "            TP = int(list_training[12][-1])\n",
    "            TN = int(list_training[14][-1])\n",
    "            FP = int(list_training[16][-1])\n",
    "            FN = int(list_training[18][-1])\n",
    "\n",
    "            specificity = TP/(TP+FN)\n",
    "            sensitivity = TN/(TN+FP)\n",
    "\n",
    "            epochs = list(range(1,len(val_accuracies)))\n",
    "\n",
    "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "        print(netname, batch_size, loss_fn, lr, '\\t', optimizer, '\\t|\\t', pretrained, '\\t|\\t', epoch_nr, '\\t|\\t', val_accuracies[2][0:4], '\\t|\\t', str(specificity)[0:4], '\\t|\\t', str(sensitivity)[0:4], '\\t|', preprocessing)\n",
    "        #print(\"%.2f\" % (float(val_accuracies[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we have implemented early stopping, as well as a scheduler. We've also tried to use pretrained models on which we've done finetunning. We've also changed the data-preprocessing to get images for which the aspect ratio is preserved. The SGD optimizer was setup to use momentum.\n",
    "\n",
    "We notice that using non-deformed images (i.e. padded images) provides us with better results. Also, the pretraining of the model does not have a huge impact on the performances. This was to be expected since the pretrained values available in pytorch do not come from medical (skin) images.\n",
    "\n",
    "From the above results, we find that the two following combination give us the best accuracy values:\n",
    "\n",
    "ResNet18  bs=8 lr=0.001 SGD \t|\t True \t|\t 6 \t|\t 91.0 \t|\t 0.42 \t|\t 0.96 \t| padded\n",
    "\n",
    "ResNet18  bs=8 lr=0.001 Adam \t|\t False \t|\t 6 \t|\t 90.7 \t|\t 0.5 \t|\t 0.95 \t| padded\n",
    "\n",
    "SGD with momentum give us the best accuracy, but Adam provides the highest sensitivity.\n",
    "\n",
    "From here, we've looked at a solution to improve the specificity of the model, namely by giving more weights to one of the class over the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_plot = []\n",
    "files_plot.append(files_[4])\n",
    "files_plot.append(files_[6])\n",
    "\n",
    "curve_properties = ['xb-','xr-']\n",
    "\n",
    "idx = 0\n",
    "for f in files_plot:\n",
    "    file_name = f.rsplit('/')[-1]\n",
    "    parameters = file_name.split('_')\n",
    "    netname = parameters[1]\n",
    "    batch_size = parameters[2]\n",
    "    loss_fn = parameters[3]\n",
    "    lr = parameters[4]\n",
    "    optimizer = parameters[5]\n",
    "    epochs_s = parameters[6]\n",
    "    if lr == \"lr=0.001\" and batch_size==\"bs=8\":\n",
    "        with open(f) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            list_training = []\n",
    "            for row in csv_reader:\n",
    "                list_training.append(row)\n",
    "            train_losses = list_training[0]\n",
    "            train_accuracies = list_training[1]\n",
    "            val_losses = list_training[2]\n",
    "            val_accuracies = list_training[3]\n",
    "\n",
    "            time = list_training[4]\n",
    "\n",
    "            TP = int(list_training[5][-1])\n",
    "            TN = int(list_training[6][-1])\n",
    "            FP = int(list_training[7][-1])\n",
    "            FN = int(list_training[8][-1])\n",
    "\n",
    "            specificity = TP/(TP+FN)\n",
    "            sensitivity = TN/(TN+FP)\n",
    "\n",
    "            epochs = list(range(1,len(val_accuracies)))\n",
    "\n",
    "        #val_losses = (list(np.float_(val_losses[1:])))\n",
    "        val_accuracies = (list(np.float_(val_accuracies[1:])))\n",
    "        print(val_losses)\n",
    "        epochs = range(0,len(val_losses[1:]))\n",
    "        #plt.plot(val_losses,'xb-',label = \"Res-NET 50\")\n",
    "        plt.plot(val_accuracies,curve_properties[idx],label = netname+batch_size+' '+loss_fn+' '+lr+' '+optimizer)\n",
    "        plt.legend(loc=4)\n",
    "        plt.title('Training')\n",
    "        plt.xlabel('Epochs', fontsize=16)\n",
    "        plt.ylabel('Val Losses', fontsize=16)\n",
    "        idx=+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_plot = []\n",
    "files_plot.append(files[3])\n",
    "files_plot.append(files[5])\n",
    "\n",
    "curve_properties = ['xb-','xr-']\n",
    "\n",
    "idx = 0\n",
    "for f in files_plot:\n",
    "    file_name = f.rsplit('/')[-1]\n",
    "    parameters = file_name.split('_')\n",
    "    netname = parameters[0].split('-')[1]\n",
    "    batch_size = parameters[1]\n",
    "    loss_fn = parameters[2]\n",
    "    lr = parameters[3]\n",
    "    optimizer = parameters[4]\n",
    "    epochs_s = parameters[5]\n",
    "    if lr == \"lr=0.001\" and batch_size==\"bs=8\":\n",
    "        with open(f) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            list_training = []\n",
    "            for row in csv_reader:\n",
    "                list_training.append(row)\n",
    "            train_losses = list_training[0]\n",
    "            train_accuracies = list_training[2]\n",
    "            val_losses = list_training[4]\n",
    "            val_accuracies = list_training[6]\n",
    "            learning_rates = list_training[8]\n",
    "\n",
    "            time = list_training[10]\n",
    "\n",
    "            TP = int(list_training[12][-1])\n",
    "            TN = int(list_training[14][-1])\n",
    "            FP = int(list_training[16][-1])\n",
    "            FN = int(list_training[18][-1])\n",
    "\n",
    "            specificity = TP/(TP+FN)\n",
    "            sensitivity = TN/(TN+FP)\n",
    "\n",
    "            epochs = list(range(1,len(val_accuracies)))\n",
    "\n",
    "        #val_losses = (list(np.float_(val_losses[1:])))\n",
    "        val_accuracies = (list(np.float_(val_accuracies[1:])))\n",
    "        print(val_losses)\n",
    "        epochs = range(0,len(val_losses[1:]))\n",
    "        #plt.plot(val_losses,'xb-',label = \"Res-NET 50\")\n",
    "        plt.plot(val_accuracies,curve_properties[idx],label = netname+batch_size+' '+loss_fn+' '+lr+' '+optimizer)\n",
    "        plt.legend(loc=4)\n",
    "        plt.title('Training')\n",
    "        plt.xlabel('Epochs', fontsize=16)\n",
    "        plt.ylabel('Val Losses', fontsize=16)\n",
    "        idx=+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve analysis\n",
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_load = glob.glob(modelstate_dir + \"/_*.pth\", recursive=True)\n",
    "#print(name)\n",
    "print(models_to_load[0])\n",
    "print(models_to_load[1])\n",
    "#files = glob.glob(model_to_load, recursive=True)\n",
    "#d_state_dict(torch.load(files[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1\n",
    "# Define Model\n",
    "model_roc = models.resnet18()\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "optimizer = optimizer = torch.optim.Adam(model_roc.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model_roc = model_roc.to(device)\n",
    "\n",
    "model_roc.load_state_dict(torch.load(models_to_load[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2\n",
    "# Define Model\n",
    "model_roc2 = models.resnet18()\n",
    "num_ftrs = model.fc.in_features  \n",
    "model_roc2.fc = nn.Linear(num_ftrs, 2)\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "optimizer = optimizer = torch.optim.Adam(model_roc2.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model_roc2 = model_roc2.to(device)\n",
    "\n",
    "model_roc2.load_state_dict(torch.load(models_to_load[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Loaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curves for model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificities1 = []\n",
    "sensitivities1 = []\n",
    "accuracies1 = []\n",
    "\n",
    "balances = np.arange(0.46,0.54,0.01)\n",
    "\n",
    "for balance in np.nditer(balances):\n",
    "    print(type(float(balance)))\n",
    "    val_loss, val_accuracy, TP, TN, FP, FN  = test_.test(model_roc, dataloaders['test'], loss_fn, device, balance=float(balance))\n",
    "    specificities1.append(TP/(TP+FN))\n",
    "    sensitivities1.append(TN/(TN+FP))\n",
    "    accuracies1.append(val_accuracy)\n",
    "\n",
    "specificities1_np = np.asarray(specificities1)\n",
    "sensitivities1_np = np.asarray(sensitivities1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificities2 = []\n",
    "sensitivities2 = []\n",
    "accuracies2 = []\n",
    "\n",
    "balances = np.arange(0.46,0.54,0.01)\n",
    "\n",
    "for balance in np.nditer(balances):\n",
    "    print(type(float(balance)))\n",
    "    val_loss, val_accuracy, TP, TN, FP, FN  = test_.test(model_roc2, dataloaders['test'], loss_fn, device, balance=float(balance))\n",
    "    specificities2.append(TP/(TP+FN))\n",
    "    sensitivities2.append(TN/(TN+FP))\n",
    "    accuracies2.append(val_accuracy)\n",
    "\n",
    "specificities2_np = np.asarray(specificities2)\n",
    "sensitivities2_np = np.asarray(sensitivities2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(1-specificities1_np,sensitivities1_np,'xb-',label = \"Res-NET 18\")\n",
    "plt.plot(1-specificities2_np,sensitivities2_np,'xr-',label = \"other model\")\n",
    "plt.legend(loc=4)\n",
    "plt.title('ROC')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xlabel('1-Specificity', fontsize=16)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel('Sensitivity', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
