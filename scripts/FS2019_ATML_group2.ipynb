{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATML Project Report\n",
    "\n",
    "### Group 2\n",
    "Members: LÃ©onard Barras & Nathan Gyger\n",
    "\n",
    "Github: https://github.com/svapili/FS2019_ATML_Group2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melanoma image classification\n",
    "### Goal:\n",
    "Implement a deep learning algorithm to classify mole pictures as benign or malignant using the [ISIC database](https://isic-archive.com/).\n",
    "\n",
    "### Approach description:\n",
    "\n",
    "- Class imbalance => data augmentation\n",
    "- Images => CNN as a logical choice\n",
    "- Transfer learning\n",
    "\n",
    "### Procedure\n",
    "\n",
    "- Augment the data for better balancing of classes (50-50 /33-67 ?)\n",
    "- Train different CNN on a few epochs with different optimizer, learning rate and batch sizes.\n",
    "- Select the more performant regarding different metrics: Accuracy, TP, TN ... Be sure that the problem of class unbalance has been corrected\n",
    "- ? Fine tune ?\n",
    "- ? Modify the argmax threshold in the output, Plot ROC Curve for different models ?\n",
    "- Modify data augmentation for getting a better specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "TODO: add accuracy table for different parameters\n",
    "\n",
    "## Learning curve\n",
    "TODO: add graphic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Hello from cluster!\")\n",
    "print(\"Available GPU: \", torch.cuda.get_device_name(0))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import util\n",
    "import csv\n",
    "import glob\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom functions\n",
    "import dataSplitter\n",
    "import loader\n",
    "import dataAugmenter\n",
    "import SimpleNet\n",
    "import train\n",
    "import test_\n",
    "import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths definitions\n",
    "cluster = False\n",
    "if cluster:\n",
    "    Path = '/var/tmp/'\n",
    "else:\n",
    "    Path = '../data/'\n",
    "dataDir = Path + 'ISIC-images'\n",
    "trainDir = Path + 'ISIC-images/train/'\n",
    "testDir = Path + 'ISIC-images/test/'\n",
    "valDir = Path + 'ISIC-images/val/'\n",
    "\n",
    "\n",
    "# Paths definitions for saving results and model state\n",
    "my_path = os.getcwd()\n",
    "dir = os.path.dirname(my_path)\n",
    "results_dir = dir + '/results'\n",
    "modelstate_dir = '/var/tmp/modelstate'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "if not os.path.exists(modelstate_dir):\n",
    "    os.makedirs(modelstate_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "newDataSplit = False # Set to true to split the data randomly again. Data have first to be downloaded and extracted with data_extractor.py\n",
    "dataPreprocessing = False # Set to true to resize and augment the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we can use CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a new random split of the data\n",
    "# Data have first to be downloaded and extracted with data_extractor.py\n",
    "if (newDataSplit):\n",
    "    testRatio = .1\n",
    "    valRatio = .1\n",
    "    split(trainDir, testDir, valDir, testRatio, valRatio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data (resizing and augmenting)\n",
    "if (dataPreprocessing):\n",
    "    dataAugmenter.preprocessData([trainDir, testDir, valDir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloaders objects\n",
    "image_datasets, dataloaders = loader.melanomaDataLoader(dataDir, batch_size=batch_size)\n",
    "\n",
    "# Get dataset objects sizes\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test', 'val']}\n",
    "print(\"Size of the dataset objects: \", dataset_sizes)\n",
    "\n",
    "# Get the class names\n",
    "class_names = image_datasets['train'].classes\n",
    "print(\"Images class names: \", class_names)\n",
    "\n",
    "# Visualize sample images\n",
    "print(\"Sample images:\")\n",
    "loader.showSample(dataloaders, dataset_sizes, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# SELECT MODEL\n",
    "###############################\n",
    "pretrained = False\n",
    "\n",
    "#model = models.AlexNet(num_classes=2)\n",
    "model = models.resnet18(pretrained=pretrained) \n",
    "#model = models.resnet50(pretrained=pretrained) \n",
    "\n",
    "###############################\n",
    "# SELECT OPTIMIZER\n",
    "###############################\n",
    "finetune = True # Only has an effect if pretrained==True. If finetune==False, then fixed-feature\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer_name=\"Adam\" # \"Adam\" or \"SGD\"\n",
    "        \n",
    "optimizer = optimize.setup_optimizer(model, learning_rate, optimizer_name, pretrained, finetune)\n",
    "\n",
    "###############################\n",
    "# SELECT SCHEDULER\n",
    "###############################\n",
    "schedule = True\n",
    "step_size = 5\n",
    "gamma = 0.1\n",
    "\n",
    "if schedule:\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma)\n",
    "else:\n",
    "    scheduler = None\n",
    "    \n",
    "###############################\n",
    "# SELECT EARLY STOPPING\n",
    "###############################\n",
    "earlyStop = True\n",
    "\n",
    "if earlyStop:\n",
    "    best_val_loss = np.inf\n",
    "    best_model = None\n",
    "    max_epochs = 5 # if no improvement after 5 epochs, stop training\n",
    "    counter = 0\n",
    "\n",
    "###############################\n",
    "# SELECT LOSS FUNCTION\n",
    "###############################\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 20\n",
    "debug_training_status = True\n",
    "\n",
    "\n",
    "saving = True\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Epoch Training Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test train and test function\n",
    "if debug_training_status is True:\n",
    "    train_loss, train_accuracy = train.train(model, dataloaders['train'], optimizer, loss_fn, device, status = debug_training_status)\n",
    "    val_loss, val_accuracy, TP, TN, FP, FN  = test_.test(model, dataloaders['val'], loss_fn, device)\n",
    "    test_loss, test_accuracy, TP, TN, FP, FN = test_.test(model, dataloaders['test'], loss_fn, device)\n",
    "    print('Test training: train_loss: {:.4f}, train_accuracy: {:.4f}, val_loss: {:.4f}, val_accuracy: {:.4f}, test_loss: {:.4f}, test_accuracy: {:.4f}'.format(\n",
    "        train_loss,\n",
    "        train_accuracy,\n",
    "        val_loss,\n",
    "        val_accuracy,\n",
    "        test_loss,\n",
    "        test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import time\n",
    "    \n",
    "    train_losses, train_accuracies = ['train_losses'], ['train_accuracies']\n",
    "    val_losses, val_accuracies = ['val_losses'], ['val_accuracies']\n",
    "    learn_rates = ['learning_rate']\n",
    "    time_epoch = ['execution time']\n",
    "    \n",
    "    TPs = ['True Positives']\n",
    "    TNs = ['True Negatives']\n",
    "    FPs = ['False Positives']\n",
    "    FNs = ['False Negatives']\n",
    "    \n",
    "    config  = model._get_name() + \" \" + \"_bs=\" + str(batch_size)\n",
    "    \n",
    "    ##############################\n",
    "    # Training Epochs            #\n",
    "    ##############################\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time_epoch = time.time()\n",
    "        \n",
    "        train_loss, train_accuracy = train.train(model, dataloaders['train'], optimizer, loss_fn, device)\n",
    "        val_loss, val_accuracy, TP, TN, FP, FN  = test_.test(model, dataloaders['val'], loss_fn, device) \n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        TPs.append(TP)\n",
    "        TNs.append(TN)\n",
    "        FPs.append(FP)\n",
    "        FNs.append(FN)\n",
    "        \n",
    "        # SCHEDULER\n",
    "        learn_rates.append(optimizer.param_groups[0]['lr'])\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # TIME CALCULATION\n",
    "        time_last_epoch = time.time() - start_time_epoch\n",
    "        time_epoch.append(time_last_epoch)\n",
    "        \n",
    "        # OTHER METRICS\n",
    "        \n",
    "        \n",
    "        print('Epoch {}/{}: train_loss: {:.4f}, train_accuracy: {:.4f}, val_loss: {:.4f}, val_accuracy: {:.4f}, learn_rates: {:}, epoch execution time: {:.4f}'.format(\n",
    "            epoch + 1, n_epochs,\n",
    "            train_losses[-1],\n",
    "            train_accuracies[-1],\n",
    "            val_losses[-1],\n",
    "            val_accuracies[-1],\n",
    "            learn_rates[-1],\n",
    "            time_epoch[-1]))\n",
    "        \n",
    "        print('True Positive: {}, True Negative: {}, False Positives: {}, False Negative: {}'.format(\n",
    "            TPs[-1],\n",
    "            TNs[-1],\n",
    "            FPs[-1],\n",
    "            FNs[-1]))\n",
    "\n",
    "    ##############################\n",
    "    # Saving results             #\n",
    "    ##############################\n",
    "\n",
    "        if saving is True: #and (epoch+1) % 5 == 0:\n",
    "            print('...saving...')\n",
    "            name = config + '_' + loss_fn.__str__() + '_lr=' + str(learning_rate) + '_' +(optimizer.__str__()).split(' ')[0]\n",
    "\n",
    "            #remove old results\n",
    "            for filename in glob.glob(results_dir + '/' + name + '*'):\n",
    "                os.remove(filename)\n",
    "            for filename in glob.glob(modelstate_dir + '/' + name + '*'):\n",
    "                os.remove(filename)\n",
    "\n",
    "            name = name + '_Epoch_' + str(epoch+1)\n",
    "\n",
    "            # save model weights\n",
    "            torch.save(model.state_dict(), modelstate_dir + '/' + name + '.pth')\n",
    "\n",
    "            # save results per epoch\n",
    "            path = results_dir + '/' + name + '.csv'\n",
    "            with open(path, 'a') as csvFile:\n",
    "                writer = csv.writer(csvFile)\n",
    "                writer.writerow(train_losses)\n",
    "                writer.writerow(train_accuracies)\n",
    "                writer.writerow(val_losses)\n",
    "                writer.writerow(val_accuracies)\n",
    "                writer.writerow(learn_rates)\n",
    "                writer.writerow(time_epoch)\n",
    "                writer.writerow(TPs)\n",
    "                writer.writerow(TNs)\n",
    "                writer.writerow(FPs)\n",
    "                writer.writerow(FNs)\n",
    "            csvFile.close()\n",
    "            \n",
    "    ##############################\n",
    "    # Early stopping             #\n",
    "    ##############################\n",
    "        if earlyStop:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = deepcopy(model)\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "            if counter == max_epochs:\n",
    "                print('No improvement for {} epochs; training stopped.'.format(max_epochs))\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(results_dir + \"/_*.csv\", recursive=True)\n",
    "files.sort()\n",
    "print('Configuration \\t'.expandtabs(60), 'Accuracy \\t' , 'Specificity \\t Sensitivity')\n",
    "for f in files:\n",
    "    file_name = f.rsplit('/')[-1]\n",
    "    parameters = file_name.split('_')\n",
    "    netname = parameters[1]\n",
    "    batch_size = parameters[2]\n",
    "    loss_fn = parameters[3]\n",
    "    lr = parameters[4]\n",
    "    optimizer = parameters[5]\n",
    "    epochs_s = parameters[6]\n",
    "    if lr == \"lr=0.001\" and batch_size==\"bs=8\":\n",
    "        with open(f) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            list_training = []\n",
    "            for row in csv_reader:\n",
    "                list_training.append(row)\n",
    "            train_losses = list_training[0]\n",
    "            train_accuracies = list_training[1]\n",
    "            val_losses = list_training[2]\n",
    "            val_accuracies = list_training[3]\n",
    "\n",
    "            time = list_training[4]\n",
    "\n",
    "            TP = int(list_training[5][-1])\n",
    "            TN = int(list_training[6][-1])\n",
    "            FP = int(list_training[7][-1])\n",
    "            FN = int(list_training[8][-1])\n",
    "\n",
    "            specificity = TP/(TP+FN)\n",
    "            sensitivity = TN/(TN+FP)\n",
    "\n",
    "            epochs = list(range(1,len(val_accuracies)))\n",
    "\n",
    "        print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "        print(netname, batch_size, loss_fn, lr, optimizer, epochs_s, '\\t|\\t', val_accuracies[2][0:4], '\\t|\\t', str(specificity)[0:4], '\\t|\\t', str(sensitivity)[0:4])\n",
    "        #print(\"%.2f\" % (float(val_accuracies[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_plot = []\n",
    "files_plot.append(files[5])\n",
    "files_plot.append(files[8])\n",
    "\n",
    "curve_properties = ['xb-','xr-']\n",
    "\n",
    "idx = 0\n",
    "for f in files_plot:\n",
    "    file_name = f.rsplit('/')[-1]\n",
    "    parameters = file_name.split('_')\n",
    "    netname = parameters[1]\n",
    "    batch_size = parameters[2]\n",
    "    loss_fn = parameters[3]\n",
    "    lr = parameters[4]\n",
    "    optimizer = parameters[5]\n",
    "    epochs_s = parameters[6]\n",
    "    if lr == \"lr=0.001\" and batch_size==\"bs=8\":\n",
    "        with open(f) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            list_training = []\n",
    "            for row in csv_reader:\n",
    "                list_training.append(row)\n",
    "            train_losses = list_training[0]\n",
    "            train_accuracies = list_training[1]\n",
    "            val_losses = list_training[2]\n",
    "            val_accuracies = list_training[3]\n",
    "\n",
    "            time = list_training[4]\n",
    "\n",
    "            TP = int(list_training[5][-1])\n",
    "            TN = int(list_training[6][-1])\n",
    "            FP = int(list_training[7][-1])\n",
    "            FN = int(list_training[8][-1])\n",
    "\n",
    "            specificity = TP/(TP+FN)\n",
    "            sensitivity = TN/(TN+FP)\n",
    "\n",
    "            epochs = list(range(1,len(val_accuracies)))\n",
    "\n",
    "        #val_losses = (list(np.float_(val_losses[1:])))\n",
    "        val_accuracies = (list(np.float_(val_accuracies[1:])))\n",
    "        print(val_losses)\n",
    "        epochs = range(0,len(val_losses[1:]))\n",
    "        #plt.plot(val_losses,'xb-',label = \"Res-NET 50\")\n",
    "        plt.plot(val_accuracies,curve_properties[idx],label = netname+batch_size+' '+loss_fn+' '+lr+' '+optimizer)\n",
    "        plt.legend(loc=4)\n",
    "        plt.title('Training')\n",
    "        plt.xlabel('Epochs', fontsize=16)\n",
    "        plt.ylabel('Val Losses', fontsize=16)\n",
    "        idx=+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve analysis\n",
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0d27ee6dcaf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels_to_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelstate_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/_*.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_to_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "models_to_load = glob.glob(modelstate_dir + \"/_*.pth\", recursive=True)\n",
    "print\n",
    "#print(name)\n",
    "print(models_to_load)\n",
    "files = glob.glob(model_to_load, recursive=True)\n",
    "d_state_dict(torch.load(files[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1\n",
    "# Define Model\n",
    "model_roc = models.resnet50()\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "optimizer = optimizer = torch.optim.Adam(model_roc.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model_roc = model_roc.to(device)\n",
    "\n",
    "model_roc.load_state_dict(torch.load(models_to_load[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2\n",
    "# Define Model\n",
    "model_roc = models.resnet50()\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "optimizer = optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model_roc.load_state_dict(torch.load(models_to_load[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Loaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curves for model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    specificities1 = []\n",
    "    sensitivities1 = []\n",
    "    accuracies1 = []\n",
    "\n",
    "    balances = np.arange(0.46,0.54,0.01)\n",
    "\n",
    "    for balance in np.nditer(balances):\n",
    "        print(type(float(balance)))\n",
    "        val_loss, val_accuracy, TP, TN, FP, FN  = test_.test(model_roc, dataloaders['test'], loss_fn, device, balance=float(balance))\n",
    "        specificities1.append(TP/(TP+FN))\n",
    "        sensitivities1.append(TN/(TN+FP))\n",
    "        accuracies1.append(val_accuracy)\n",
    "    \n",
    "    specificities1_np = np.asarray(specificities1)\n",
    "    sensitivities1_np = np.asarray(sensitivities1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    specificities2 = []\n",
    "    sensitivities2 = []\n",
    "    accuracies2 = []\n",
    "\n",
    "    balances = np.arange(0.46,0.54,0.01)\n",
    "\n",
    "    for balance in np.nditer(balances):\n",
    "        print(type(float(balance)))\n",
    "        val_loss, val_accuracy, TP, TN, FP, FN  = test_.test(model_roc, dataloaders['test'], loss_fn, device, balance=float(balance))\n",
    "        specificities1.append(TP/(TP+FN))\n",
    "        sensitivities1.append(TN/(TN+FP))\n",
    "        accuracies1.append(val_accuracy)\n",
    "    \n",
    "    specificities2_np = np.asarray(specificities2)\n",
    "    sensitivities2_np = np.asarray(sensitivities2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.plot(1-specificities1_np,sensitivities1_np,'xb-',label = \"Res-NET 50\")\n",
    "    #plt.plot(1-specificities2_np,sensitivities2_np,'xr-',label = \"other model\")\n",
    "    plt.legend(loc=4)\n",
    "    plt.title('ROC')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.xlabel('1-Specificity', fontsize=16)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.ylabel('Sensitivity', fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
